{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fKdXs1tze3sU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4076b1e-3685-44da-bccc-833b4c396dd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [1 InRelease 43.1 kB/110 k\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to apt.kitware\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.81)] [Connecting to apt.kitware\u001b[0m\r                                                                               \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://apt.kitware.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [49.9 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,419 kB]\n",
            "Fetched 1,810 kB in 2s (975 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "23 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://apt.kitware.com/ubuntu/dists/bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "autoconf is already the newest version (2.71-2).\n",
            "bison is already the newest version (2:3.8.2+dfsg-1build1).\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "flex is already the newest version (2.6.4-8build2).\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "libtool is already the newest version (2.4.6-15build2).\n",
            "pkg-config is already the newest version (0.29.2-1ubuntu3).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04).\n",
            "python3-numpy is already the newest version (1:1.21.5-1ubuntu22.04.1).\n",
            "python3-pip is already the newest version (22.0.2+dfsg-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "Repository: 'deb https://apt.kitware.com/ubuntu/ bionic main'\n",
            "Description:\n",
            "Archive for codename: bionic components: main\n",
            "More info: https://apt.kitware.com/ubuntu/\n",
            "Adding repository.\n",
            "Press [ENTER] to continue or Ctrl-c to cancel.\n",
            "Found existing deb entry in /etc/apt/sources.list.d/archive_uri-https_apt_kitware_com_ubuntu_-jammy.list\n",
            "Adding deb entry to /etc/apt/sources.list.d/archive_uri-https_apt_kitware_com_ubuntu_-jammy.list\n",
            "Found existing deb-src entry in /etc/apt/sources.list.d/archive_uri-https_apt_kitware_com_ubuntu_-jammy.list\n",
            "Adding disabled deb-src entry to /etc/apt/sources.list.d/archive_uri-https_apt_kitware_com_ubuntu_-jammy.list\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://apt.kitware.com/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "W: https://apt.kitware.com/ubuntu/dists/bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://apt.kitware.com/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "W: https://apt.kitware.com/ubuntu/dists/bionic/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Some packages could not be installed. This may mean that you have\n",
            "requested an impossible situation or if you are using the unstable\n",
            "distribution that some required packages have not yet been created\n",
            "or been moved out of Incoming.\n",
            "The following information may help to resolve the situation:\n",
            "\n",
            "The following packages have unmet dependencies:\n",
            " cmake : Depends: libssl1.1 (>= 1.1.1) but it is not installable\n",
            "E: Unable to correct problems, you have held broken packages.\n",
            "/bin/bash: line 1: --version: command not found\n",
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Requirement already satisfied: nle in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle) (2.11.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from nle) (1.23.5)\n",
            "Requirement already satisfied: gym<=0.23,>=0.15 in /usr/local/lib/python3.10/dist-packages (from nle) (0.23.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23,>=0.15->nle) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23,>=0.15->nle) (0.0.8)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libcairo2 is already the newest version (1.16.0-5ubuntu2).\n",
            "libcairo2-dev is already the newest version (1.16.0-5ubuntu2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "sox is already the newest version (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n",
            "Requirement already satisfied: manimlib in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.5.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: minihack in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Collecting argparse (from manimlib)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: colour in /usr/local/lib/python3.10/dist-packages (from manimlib) (0.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from manimlib) (1.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from manimlib) (9.4.0)\n",
            "Requirement already satisfied: progressbar in /usr/local/lib/python3.10/dist-packages (from manimlib) (2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from manimlib) (1.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from manimlib) (4.66.1)\n",
            "Requirement already satisfied: pycairo in /usr/local/lib/python3.10/dist-packages (from manimlib) (1.25.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from manimlib) (0.25.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from manimlib) (2.16.1)\n",
            "Requirement already satisfied: gym<=0.23,>=0.15 in /usr/local/lib/python3.10/dist-packages (from minihack) (0.23.0)\n",
            "Requirement already satisfied: nle==0.9.0 in /usr/local/lib/python3.10/dist-packages (from minihack) (0.9.0)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from nle==0.9.0->minihack) (2.11.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23,>=0.15->minihack) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23,>=0.15->minihack) (0.0.8)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y build-essential autoconf libtool pkg-config python3-dev \\\n",
        "    python3-pip python3-numpy git flex bison libbz2-dev\n",
        "\n",
        "!wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | sudo apt-key add -\n",
        "!sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main'\n",
        "!sudo apt-get update && apt-get --allow-unauthenticated install -y \\\n",
        "    cmake \\\n",
        "    kitware-archive-keyring\n",
        "\n",
        "!sudo rm $(which cmake)\n",
        "!$(which cmake) --version\n",
        "\n",
        "!pip3 install -Uv nle\n",
        "!apt-get install sox ffmpeg libcairo2 libcairo2-dev\n",
        "!pip install manimlib pygame opencv-python minihack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1iPtEtHoHEcR"
      },
      "outputs": [],
      "source": [
        "# https://github.com/BY571/SAC_discrete/blob/main/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fPGDPTL85dbQ"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spLWye2_FP0b"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "g7XduPH6FGl8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import minihack\n",
        "\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hyperparams):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        # Initialize fully connected layers for glyph output after convolutional and pooling layers\n",
        "        self.hidden_size = hyperparams[\"hidden_size\"]\n",
        "        self.fc1 = Linear(state_size, out_features=self.hidden_size)\n",
        "        self.fc2 = Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = Linear(in_features=self.hidden_size, out_features=action_size)\n",
        "        # To calculate the probability of taking each action in the given state\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, state):\n",
        "       # Transform the glyph and state arrays into tensors\n",
        "\n",
        "        if (type(state) is dict): # if state is just a dict with 2 keys (glyphs, message)\n",
        "          message_t  = torch.from_numpy(state[\"message\"]).float().to(device)\n",
        "          glyphs_t  = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "\n",
        "          glyphs_t = glyphs_t.reshape((1,1659))\n",
        "\n",
        "\n",
        "        else: # if state is a batch - here, state is an array of dicts - if you call agent.learn()\n",
        "            glyphs_t  = torch.from_numpy(np.array([ s[\"glyphs\"] for s in state])).float().to(device)\n",
        "            message_t  = torch.from_numpy(np.array([ s[\"message\"] for s in state])).float().to(device)\n",
        "\n",
        "            glyphs_t = torch.squeeze(glyphs_t, 1) # remove all dimensions with 1\n",
        "            message_t = torch.squeeze(message_t, 1) # remove all dimensions with 1\n",
        "\n",
        "            # print(glyphs_t.shape)\n",
        "            glyphs_t = glyphs_t.reshape((256,1659))\n",
        "\n",
        "        # Combine glyphs output from convolution and fully connected layers\n",
        "        # with message output from fully connected layer\n",
        "        # Cat and Concat are used for different versions of PyTorch\n",
        "\n",
        "        try:\n",
        "            combined = torch.cat((glyphs_t,message_t),1)\n",
        "        except:\n",
        "            combined = torch.concat([glyphs_t,message_t],1)\n",
        "\n",
        "        # Pass glyphs and messaged combination through a fully connected layer\n",
        "\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_probs = self.softmax(self.fc3(x))\n",
        "        return action_probs\n",
        "\n",
        "    def evaluate(self, state, epsilon=1e-6):\n",
        "        action_probs = self.forward(state)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n",
        "        z = action_probs == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probs + z)\n",
        "        return action.detach().cpu(), action_probs, log_action_probabilities\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
        "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
        "        \"\"\"\n",
        "        action_probs = self.forward(state)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample().to(device)\n",
        "        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n",
        "        z = action_probs == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probs + z)\n",
        "        return action.detach().cpu(), action_probs, log_action_probabilities\n",
        "\n",
        "    def get_det_action(self, state):\n",
        "        action_probs = self.forward(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample().to(device)\n",
        "        return action.detach().cpu()\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hyperparams):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in the network layers\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.hidden_size = hyperparams[\"hidden_size\"]\n",
        "        # Initialize fully connected layers for glyph output after convolutional and pooling layers\n",
        "        self.fc1 = Linear(state_size, out_features=self.hidden_size)\n",
        "        self.fc2 = Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = Linear(in_features=self.hidden_size, out_features=action_size)\n",
        "        # To calculate the probability of taking each action in the given state\n",
        "        # self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "       # Transform the glyph and state arrays into tensors\n",
        "\n",
        "        if (type(state) is dict): # if state is just a dict with 2 keys (glyphs, message)\n",
        "          message_t  = torch.from_numpy(state[\"message\"]).float().to(device)\n",
        "          glyphs_t  = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "\n",
        "          glyphs_t = glyphs_t.reshape((1,1659))\n",
        "\n",
        "        else: # if state is a batch - here, state is an array of dicts - if you call agent.learn()\n",
        "            glyphs_t  = torch.from_numpy(np.array([ s[\"glyphs\"] for s in state])).float().to(device)\n",
        "            message_t  = torch.from_numpy(np.array([ s[\"message\"] for s in state])).float().to(device)\n",
        "\n",
        "            glyphs_t = torch.squeeze(glyphs_t, 1) # remove all dimensions with 1\n",
        "            message_t = torch.squeeze(message_t, 1) # remove all dimensions with 1\n",
        "\n",
        "            glyphs_t = glyphs_t.reshape((256,1659))\n",
        "\n",
        "        # Combine glyphs output from convolution and fully connected layers\n",
        "        # with message output from fully connected layer\n",
        "        # Cat and Concat are used for different versions of PyTorch\n",
        "        try:\n",
        "            combined = torch.cat((glyphs_t,message_t),1)\n",
        "        except:\n",
        "            combined = torch.concat([glyphs_t,message_t],1)\n",
        "\n",
        "        # Pass glyphs and messaged combination through a fully connected layer\n",
        "\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rgIgZ6XFUTu"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "qI_KDyJpFWQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import copy\n",
        "\n",
        "\n",
        "class SAC(nn.Module):\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                        state_size,\n",
        "                        action_size,\n",
        "                        hyperparams,\n",
        "                        device\n",
        "                ):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        super(SAC, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.gamma = hyperparams[\"discount\"]\n",
        "        self.tau = hyperparams[\"interpolation_factor\"]\n",
        "        hidden_size = hyperparams[\"hidden_size\"]\n",
        "        learning_rate = hyperparams[\"lr\"]\n",
        "        self.clip_grad_param = hyperparams[\"clip_grad_param\"]\n",
        "\n",
        "        self.target_entropy = -action_size  # -dim(A)\n",
        "\n",
        "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
        "        self.alpha = self.log_alpha.exp().detach()\n",
        "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=learning_rate)\n",
        "\n",
        "        # Actor Network\n",
        "\n",
        "        self.actor_local = Actor(state_size, action_size, hyperparams).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "\n",
        "        self.critic1 = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic2 = Critic(state_size, action_size, hyperparams).to(device)\n",
        "\n",
        "        assert self.critic1.parameters() != self.critic2.parameters()\n",
        "\n",
        "        self.critic1_target = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "\n",
        "        self.critic2_target = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        # state = torch.from_numpy(state).float().to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local.get_det_action(state)\n",
        "        return action.numpy()\n",
        "\n",
        "    def calc_policy_loss(self, states, alpha):\n",
        "        _, action_probs, log_pis = self.actor_local.evaluate(states)\n",
        "\n",
        "        q1 = self.critic1(states)\n",
        "        q2 = self.critic2(states)\n",
        "        min_Q = torch.min(q1,q2)\n",
        "        actor_loss = (action_probs * (alpha * log_pis - min_Q )).sum(1).mean()\n",
        "        log_action_pi = torch.sum(log_pis * action_probs, dim=1)\n",
        "        return actor_loss, log_action_pi\n",
        "\n",
        "    def learn(self, step, experiences, gamma, d=1):\n",
        "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
        "        Q_targets = r + γ * (min_critic_target(next_state, actor_target(next_state)) - α *log_pi(next_action|next_state))\n",
        "        Critic_loss = MSE(Q, Q_target)\n",
        "        Actor_loss = α * log_pi(a|s) - Q(s,a)\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        current_alpha = copy.deepcopy(self.alpha)\n",
        "        actor_loss, log_pis = self.calc_policy_loss(states, current_alpha.to(self.device))\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Compute alpha loss # α > 0 is called the temperature and determines the trade-off between received rewards and randomness of the policy\n",
        "        alpha_loss = - (self.log_alpha.exp() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean() # temperature cost function\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "        self.alpha = self.log_alpha.exp().detach()\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "        with torch.no_grad():\n",
        "            _, action_probs, log_pis = self.actor_local.evaluate(next_states)\n",
        "            Q_target1_next = self.critic1_target(next_states)\n",
        "            Q_target2_next = self.critic2_target(next_states)\n",
        "            Q_target_next = action_probs * (torch.min(Q_target1_next, Q_target2_next) - self.alpha.to(self.device) * log_pis)\n",
        "\n",
        "            # Compute Q targets for current states (y_i)\n",
        "            Q_targets = rewards + (gamma * (1 - dones) * Q_target_next.sum(dim=1).unsqueeze(-1))\n",
        "\n",
        "        # Compute critic loss\n",
        "        q1 = self.critic1(states).gather(1, actions.long()) # Gathers values along an axis specified by dim. input_tensor.gather(dim, index) . index is long tensor.\n",
        "        q2 = self.critic2(states).gather(1, actions.long()) # input and index must be same dimension\n",
        "\n",
        "        critic1_loss = 0.5 * F.mse_loss(q1, Q_targets)\n",
        "        critic2_loss = 0.5 * F.mse_loss(q2, Q_targets)\n",
        "\n",
        "        # Update critics\n",
        "        # critic 1\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        critic1_loss.backward(retain_graph=True)\n",
        "        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n",
        "        self.critic1_optimizer.step()\n",
        "        # critic 2\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic2_loss.backward()\n",
        "        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n",
        "        self.critic2_optimizer.step()\n",
        "\n",
        "        # ----------------------- update target networks ----------------------- #\n",
        "        self.soft_update(self.critic1, self.critic1_target)\n",
        "        self.soft_update(self.critic2, self.critic2_target)\n",
        "\n",
        "        return actor_loss.item(), alpha_loss.item(), critic1_loss.item(), critic2_loss.item(), current_alpha\n",
        "\n",
        "    def soft_update(self, local_model , target_model):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format state"
      ],
      "metadata": {
        "id": "SCip6rKVasiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "t9G10Jrumv6r"
      },
      "outputs": [],
      "source": [
        "def format_state(state):\n",
        "    \"\"\"Formats the state according to the input requirements of the Actor Critic Neural Network\"\"\"\n",
        "\n",
        "    # Normalize and reshape for convolutional layer input\n",
        "    glyphs = state[\"glyphs\"]\n",
        "    glyphs = glyphs/glyphs.max()\n",
        "    glyphs = glyphs.reshape((1,1,21,79))\n",
        "\n",
        "    # Normalize the message and reshape for the fully connected layer input\n",
        "    message = state[\"message\"]\n",
        "    if state[\"message\"].max()>0:\n",
        "        # Occassionally the message is empty which will cause a Zero Division error\n",
        "        message = message/message.max()\n",
        "    message = message.reshape((1,len(message)))\n",
        "\n",
        "    state = {\"glyphs\":glyphs,\"message\":message}\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxUOvK8ZINgB"
      },
      "source": [
        "Collecting samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oOqfhwG6IOto"
      },
      "outputs": [],
      "source": [
        "def collect_random(env, dataset, num_samples):\n",
        "    state = format_state(env.reset())\n",
        "    for _ in range(num_samples):\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = format_state(next_state)\n",
        "        dataset.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            state = format_state(env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHVE9whmsss"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "nk4bGu_nijPn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = np.stack([e.state for e in experiences if e is not None])\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = np.stack([e.next_state for e in experiences if e is not None])\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvl1JS2RFjRu"
      },
      "source": [
        "Training Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "mmI09_BlFiou",
        "outputId": "d0a572a4-9170-44ff-c84a-04e91306bfa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Episode: 1 | Reward: -0.2900000000000009 | Policy Loss: -6.338290214538574 | Steps: 267\n",
            "2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-9cd08f2eea42>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/minihack/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# Within this call, _is_episode_end is called and then _reward_fn,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# both using self.reward_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_episode_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nle/env/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_ascended\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnethack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhow_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnethack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mASCENDED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_observation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_in_moveloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/minihack/base.py\u001b[0m in \u001b[0;36m_get_observation\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"pixel\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minihack_obs_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             obs_dict[\"pixel\"] = self._glyph_mapper.to_rgb(\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"glyphs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/minihack/tiles/glyph_mapper.py\u001b[0m in \u001b[0;36mto_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglyphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_glyph_to_rgb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglyphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/minihack/tiles/glyph_mapper.py\u001b[0m in \u001b[0;36m_glyph_to_rgb\u001b[0;34m(self, glyphs)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import argparse\n",
        "import glob\n",
        "import random\n",
        "from nle import nethack\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "      \"run_name\": \"SAC Discrete\",\n",
        "      # \"env_name\": \"MiniHack-MazeWalk-9x9-v0\",\n",
        "      \"env_name\": \"MiniHack-Room-5x5-v0\",\n",
        "      \"episodes\": 500,\n",
        "      \"buffer_size\": int(1e6),\n",
        "      \"seed\": 42,\n",
        "      \"log_video\": 0,\n",
        "      \"save_every\": 100,\n",
        "      \"batch_size\": 256,\n",
        "      \"discount\":0.99, # discount factor gamma\n",
        "      \"lr\": 2e-4, # learning rate alpha\n",
        "      # \"lr\": 0.02, # learning rate alpha\n",
        "      \"hidden_size\": 256, # hidden layer size\n",
        "      \"interpolation_factor\": 0.005,   #tau - for soft update\n",
        "      \"clip_grad_param\":1, # gradient clipping\n",
        "      \"max_episode_steps\":1000\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "np.random.seed(hyperparams[\"seed\"])\n",
        "random.seed(hyperparams[\"seed\"])\n",
        "torch.manual_seed(hyperparams[\"seed\"])\n",
        "\n",
        "MOVE_ACTIONS = tuple(nethack.CompassDirection) + (\n",
        "                nethack.Command.OPEN,\n",
        "                nethack.Command.KICK\n",
        "            )\n",
        "env = gym.make(hyperparams[\"env_name\"],observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "        actions=MOVE_ACTIONS,max_episode_steps=hyperparams[\"max_episode_steps\"])\n",
        "\n",
        "env.seed(hyperparams[\"seed\"])\n",
        "env.action_space.seed(hyperparams[\"seed\"])\n",
        "torch.cuda.manual_seed_all(hyperparams[\"seed\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "steps = 0\n",
        "average10 = deque(maxlen=10)\n",
        "total_steps = 0\n",
        "\n",
        "agent = SAC(state_size=1915,\n",
        "                  action_size=len(MOVE_ACTIONS),\n",
        "                  hyperparams=hyperparams,\n",
        "                  device=device)\n",
        "\n",
        "\n",
        "buffer = ReplayBuffer(buffer_size=hyperparams[\"buffer_size\"], batch_size=hyperparams[\"batch_size\"], device=device )\n",
        "\n",
        "collect_random(env=env, dataset=buffer, num_samples=10000) # generate random samples\n",
        "\n",
        "rewards_arr = []\n",
        "policy_loss_arr = []\n",
        "critic1_loss_arr = []\n",
        "\n",
        "if hyperparams[\"log_video\"]:\n",
        "    env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n",
        "\n",
        "for i in range(1, hyperparams[\"episodes\"]+1):\n",
        "    state = format_state(env.reset())\n",
        "\n",
        "    episode_steps = 0\n",
        "    rewards = 0\n",
        "    print(i)\n",
        "    while True:\n",
        "        action = agent.get_action(state)\n",
        "        steps += 1\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        next_state = format_state(next_state)\n",
        "\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "        policy_loss, alpha_loss, bellmann_error1, bellmann_error2, current_alpha = agent.learn(steps, buffer.sample(), hyperparams[\"discount\"])\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "        episode_steps += 1\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    average10.append(rewards)\n",
        "    total_steps += episode_steps\n",
        "    print(\"Episode: {} | Reward: {} | Policy Loss: {} | Steps: {}\".format(i, rewards, policy_loss, steps,))\n",
        "    rewards_arr.append(rewards)\n",
        "    policy_loss_arr.append(policy_loss)\n",
        "    critic1_loss_arr.append(bellmann_error1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/rewards.txt', rewards_arr)\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/policy_loss.txt', policy_loss_arr)\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/critic1_loss.txt', critic1_loss_arr)\n",
        "print(\"total number of steps: \", steps)"
      ],
      "metadata": {
        "id": "_CSx8UPX9ExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U71v5IeQU-na"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rewards_arr)\n",
        "plt.title('Average Reward on Room-5x5-v0')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkVxGGlJVCGi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(policy_loss_arr)\n",
        "plt.title('Policy Loss on Room-5x5-v0')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(critic1_loss_arr)\n",
        "plt.title('Critic 1 Loss on Room=5x5-v0')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ny7Wk56hVoZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}