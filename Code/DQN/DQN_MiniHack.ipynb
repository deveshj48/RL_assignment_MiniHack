{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **DQN**"
      ],
      "metadata": {
        "id": "7NE1KKDFma1S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0T75r190AoB"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y build-essential autoconf libtool pkg-config python3-dev \\\n",
        "    python3-pip python3-numpy git flex bison libbz2-dev\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | sudo apt-key add -\n",
        "!sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main'\n",
        "!sudo apt-get update && apt-get --allow-unauthenticated install -y \\\n",
        "    cmake \\\n",
        "    kitware-archive-keyring"
      ],
      "metadata": {
        "id": "9kb8N1ly09HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo rm $(which cmake)\n",
        "!$(which cmake) --version"
      ],
      "metadata": {
        "id": "hw9rGhLl1CbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -Uv nle"
      ],
      "metadata": {
        "id": "w82G_Io91LwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install sox ffmpeg libcairo2 libcairo2-dev"
      ],
      "metadata": {
        "id": "P-QSWGiF1dN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install manimlib pygame opencv-python minihack"
      ],
      "metadata": {
        "id": "stD_lTiR1gBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Appropriate imports needed\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "from nle import nethack\n",
        "import minihack\n",
        "import math\n",
        "from PIL import Image\n",
        "import pygame, sys\n",
        "from pygame.locals import *\n",
        "from tensorflow import keras\n",
        "import cv2\n",
        "import random\n",
        "from gym import spaces\n",
        "from collections import deque\n",
        "import torch\n",
        "from torch import nn, optim,tensor, from_numpy, zeros, no_grad, device, cuda, save\n",
        "import torch.nn.functional as F\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "device = device(\"cuda\" if cuda.is_available() else \"cpu\") #use gpu instead for faster computation"
      ],
      "metadata": {
        "id": "QDIcqZuE1jVB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#run this cell to mount colab notebook to drive for output recording\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bfv9VoK1mQP",
        "outputId": "4926dbfc-ffcf-4634-fded-8addc2e21c4c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replay Buffer"
      ],
      "metadata": {
        "id": "wJIv2xKt1wC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from template in DQN lab\n",
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Simple storage for transitions from an environment.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        \"\"\"\n",
        "        Initialise a buffer of a given size for storing transitions\n",
        "        :param size: the maximum number of transitions that can be stored\n",
        "        \"\"\"\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._storage)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        Add a transition to the buffer. Old transitions will be overwritten if the buffer is full.\n",
        "        :param state: the agent's initial state\n",
        "        :param action: the action taken by the agent\n",
        "        :param reward: the reward the agent received\n",
        "        :param next_state: the subsequent state\n",
        "        :param done: whether the episode terminated\n",
        "        \"\"\"\n",
        "        data = (state, action, reward, next_state, done)\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "    def _encode_sample(self, indices):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "        for i in indices:\n",
        "            data = self._storage[i]\n",
        "            state, action, reward, next_state, done = data\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "        return (\n",
        "            np.array(states),\n",
        "            np.array(actions),\n",
        "            np.array(rewards),\n",
        "            np.array(next_states),\n",
        "            np.array(dones),\n",
        "        )\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"\n",
        "        Randomly sample a batch of transitions from the buffer.\n",
        "        :param batch_size: the number of transitions to sample\n",
        "        :return: a mini-batch of sampled transitions\n",
        "        \"\"\"\n",
        "        indices = np.random.randint(0, len(self._storage) - 1, size=batch_size)\n",
        "        return self._encode_sample(indices)"
      ],
      "metadata": {
        "id": "YGeqhJJS1vQj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DQN Agent"
      ],
      "metadata": {
        "id": "9t-ezdO4131p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import (BatchNorm2d, Conv2d, CrossEntropyLoss, Dropout, Linear,\n",
        "                      MaxPool2d, Module, ReLU, Sequential, Softmax)\n",
        "class DQNAgent:\n",
        "    def __init__(\n",
        "        self,\n",
        "        observation_space: spaces.Box,\n",
        "        action_space: spaces.Discrete,\n",
        "        replay_buffer: ReplayBuffer,\n",
        "        use_double_dqn,\n",
        "        lr,\n",
        "        batch_size,\n",
        "        gamma,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialise the DQN algorithm using the Adam optimiser\n",
        "        :param action_space: the action space of the environment\n",
        "        :param observation_space: the state space of the environment\n",
        "        :param replay_buffer: storage for experience replay\n",
        "        :param lr: the learning rate for Adam\n",
        "        :param batch_size: the batch size\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "\n",
        "        class DQN(nn.Module): # adapted from https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
        "\n",
        "          def __init__(self, n_observations, n_actions):\n",
        "\n",
        "            super(DQN, self).__init__()\n",
        "\n",
        "            self.fc1 = nn.Linear(in_features=81, out_features=64) #uses 81 input features as glyphs are cropped to 9x9 area\n",
        "            self.fc2 = nn.Linear(in_features=64 , out_features=32)\n",
        "            self.fc3 = nn.Linear(in_features=32 , out_features=16)\n",
        "            self.fc4 = nn.Linear(in_features=16, out_features=action_space.n)\n",
        "\n",
        "          def forward(self, x):\n",
        "              x = F.relu(self.fc1(x))\n",
        "              x = F.relu(self.fc2(x))\n",
        "              x = F.relu(self.fc3(x))\n",
        "              x = self.fc4(x)\n",
        "              return x\n",
        "\n",
        "        self.target_network = DQN(observation_space, action_space.n).to(device)\n",
        "        self.q_network = DQN(observation_space, action_space.n).to(device)\n",
        "\n",
        "\n",
        "        # initialise\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr = lr)\n",
        "        self.replay_buffer = replay_buffer\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.use_double_dqn = use_double_dqn\n",
        "\n",
        "    def optimise_td_loss(self):\n",
        "        \"\"\"\n",
        "        Optimise the TD-error over a single minibatch of transitions\n",
        "        :return: the loss\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        #   Optimise the TD-error over a single minibatch of transitions\n",
        "        #   Sample the minibatch from the replay-memory\n",
        "        #   using done (as a float) instead of if statement\n",
        "        #   return loss\n",
        "        #squeeze and unsqueeze functions used to change dimensions of variables to the appropriate ones\n",
        "        sample_batch = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        states_sample = sample_batch[0]\n",
        "        actions_sample = sample_batch[1]\n",
        "        rewards_sample = sample_batch[2]\n",
        "        next_states_sample = sample_batch[3]\n",
        "        dones_sample = sample_batch[4]\n",
        "\n",
        "        states_sample = np.array(states_sample) / 255.0\n",
        "        states_sample = from_numpy(states_sample).float().to(device)\n",
        "\n",
        "        next_states_sample = np.array(next_states_sample) / 255.0\n",
        "        next_states_sample = from_numpy(next_states_sample).float().to(device)\n",
        "\n",
        "        actions_sample = np.array(actions_sample).astype(int)\n",
        "        actions_sample = from_numpy(actions_sample).long().to(device)\n",
        "        rewards_sample = from_numpy(rewards_sample).float().to(device)\n",
        "        dones_sample = from_numpy(dones_sample).float().to(device)\n",
        "\n",
        "        with no_grad(): # reduces unnecessary computation by temporariliy disabling gradient tracking\n",
        "            if (self.use_double_dqn):\n",
        "                optimal_next_a = self.q_network(next_states_sample).max(1)[1]\n",
        "                optimal_next_q = self.target_network(next_states_sample).gather(1, optimal_next_a.unsqueeze(1)).squeeze()\n",
        "            else:\n",
        "                optimal_next_q = self.target_network(next_states_sample).max(1)[0]\n",
        "            target_q_values = rewards_sample + (((1 - dones_sample) * self.gamma)* optimal_next_q)\n",
        "\n",
        "        predicted_q = self.q_network(states_sample).gather(1, actions_sample.unsqueeze(1)).squeeze()\n",
        "\n",
        "        # Compute Huber loss\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(predicted_q, target_q_values)\n",
        "\n",
        "        # optimising params\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        for param in self.q_network.parameters():\n",
        "          param.grad.data.clamp_(-1, 1) # clamp restricts values of gradient tensor to be between -1 and 1 - form of gradient clipping to prevent 'exploding gradients'\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        del states_sample\n",
        "        del next_states_sample\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        \"\"\"\n",
        "        Update the target Q-network by copying the weights from the current Q-network\n",
        "        \"\"\"\n",
        "        # TODO update target_network parameters with policy_network parameters\n",
        "\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict()) # copies parameters\n",
        "\n",
        "\n",
        "    def act(self, state: np.ndarray):\n",
        "        \"\"\"\n",
        "        Select an action greedily from the Q-network given the state\n",
        "        :param state: the current state\n",
        "        :return: the action to take\n",
        "        \"\"\"\n",
        "        state=np.array(state)/255.0\n",
        "        state=from_numpy(state).float()\n",
        "        state=state.unsqueeze(0).to(device)\n",
        "        with no_grad():\n",
        "            action = self.q_network(state).max(1)[1]\n",
        "            return action.item()\n",
        "\n",
        "    def save_model(self, path): #function for saving model\n",
        "      save(self.q_network.state_dict(), path+\"maze_final5_dqn_q_weights_final.pth\")\n",
        "      save(self.target_network.state_dict(), path+\"maze_final5_dqn_target_weights_final.pth\")\n"
      ],
      "metadata": {
        "id": "6z0S46ip12Hn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to train the DQN\n",
        "\n",
        "def TrainDQN(env, agent, max_timesteps,hyper_params,replay_buffer):\n",
        "\n",
        "  eps_timesteps = hyper_params[\"eps-fraction\"] * float(hyper_params[\"num-steps\"])\n",
        "  episode_rewards = [0.0] #records the rewards for episodes\n",
        "  loss_arr = [] #loss of episodes\n",
        "  loss = 0 #initialise\n",
        "  rewards_arr = []\n",
        "  ave_rewards_arr = []\n",
        "\n",
        "\n",
        "  state = env.reset()['glyphs_crop'].flatten() #flatten 9x9\n",
        "  for t in range(max_timesteps):\n",
        "      fraction = min(1.0, float(t) / eps_timesteps)\n",
        "      eps_threshold = hyper_params[\"eps-start\"] + fraction * (\n",
        "          hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"]\n",
        "      )\n",
        "      sample = random.random()\n",
        "      # TODO\n",
        "      #  select random action if sample is less equal than eps_threshold\n",
        "      # take step in env\n",
        "      # add state, action, reward, next_state, float(done) to reply memory - cast done to float\n",
        "      # add reward to episode_reward\n",
        "\n",
        "      if (sample <= eps_threshold):\n",
        "        action = env.action_space.sample() # random action\n",
        "      else:\n",
        "        action = agent.act(state)\n",
        "\n",
        "      next_s, reward, done, _ = env.step(action) #take the action chosen\n",
        "      next_state = next_s['glyphs_crop'].flatten()\n",
        "      replay_buffer.add(state, action, reward, next_state, float(done)) # stores experience\n",
        "      episode_rewards[-1] += reward #update the reward\n",
        "      state = next_state\n",
        "\n",
        "      if done:\n",
        "          state = env.reset()['glyphs_crop'].flatten()\n",
        "\n",
        "          mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "          #this outputs loss and reward to text files. Uncomment if needed\n",
        "          # with open(\"drive/MyDrive/dqn_videos/room_rewards_ave.txt\", 'a') as file1:\n",
        "          #   file1.write(str(mean_100ep_reward))\n",
        "          #   file1.write('\\n')\n",
        "\n",
        "          # with open(\"drive/MyDrive/dqn_videos/room_rewards_per_ep.txt\", 'a') as file1:\n",
        "          #   file1.write(str(episode_rewards[-1]))\n",
        "          #   file1.write('\\n')\n",
        "\n",
        "          # with open(\"drive/MyDrive/dqn_videos/room_loss.txt\", 'a') as file1:\n",
        "          #   file1.write(str(loss))\n",
        "          #   file1.write('\\n')\n",
        "\n",
        "          rewards_arr.append(episode_rewards[-1])\n",
        "          ave_rewards_arr.append(mean_100ep_reward)\n",
        "          loss_arr.append(loss)\n",
        "\n",
        "          episode_rewards.append(0.0)\n",
        "\n",
        "\n",
        "      if (\n",
        "          t > hyper_params[\"learning-starts\"]\n",
        "          and t % hyper_params[\"learning-freq\"] == 0\n",
        "      ):\n",
        "          loss = agent.optimise_td_loss() #calculate the loss\n",
        "\n",
        "\n",
        "      if (\n",
        "          t > hyper_params[\"learning-starts\"]\n",
        "          and t % hyper_params[\"target-update-freq\"] == 0\n",
        "      ):\n",
        "          agent.update_target_network() #update the target network\n",
        "\n",
        "      num_episodes = len(episode_rewards)\n",
        "\n",
        "      if (\n",
        "          done\n",
        "          and hyper_params[\"print-freq\"] is not None\n",
        "          and len(episode_rewards) % hyper_params[\"print-freq\"] == 0\n",
        "      ):\n",
        "          mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "          print(\"********************************************************\")\n",
        "          print(\"steps: {}\".format(t))\n",
        "          print(\"episodes: {}\".format(num_episodes))\n",
        "          print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "          print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "          print(\"********************************************************\")\n",
        "  agent.save_model()\n"
      ],
      "metadata": {
        "id": "0oj1rxoofARQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-Task 1: MiniHack-Room-5x5-v0"
      ],
      "metadata": {
        "id": "8ObYtZ1rgiwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RunDQN1():\n",
        "    hyper_params = {\n",
        "    \"seed\": 42,  # which seed to use\n",
        "    \"env\": \"MiniHack-Room-5x5-v0\",  # name of the game\n",
        "    \"replay-buffer-size\": int(5e4),  # replay buffer size\n",
        "    \"learning-rate\": 1e-3,  # learning rate for Adam optimizer\n",
        "    \"discount-factor\": 0.99,  # discount factor\n",
        "    \"num-steps\": 500000,  # total number of steps to run the environment for\n",
        "    \"batch-size\": 64,  # number of transitions to optimize at the same time\n",
        "    \"learning-starts\": 5000,  # number of steps before learning starts\n",
        "    \"learning-freq\": 2,  # number of iterations between every optimization step\n",
        "    \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "    \"target-update-freq\": 100,  # number of iterations between every target network update\n",
        "    \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "    \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "    \"eps-fraction\": 0.3,  # fraction of num-steps\n",
        "    \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    actions = tuple(nethack.CompassDirection) #actions restricted to sufficient ones needed for this environment\n",
        "\n",
        "    env = gym.make(hyper_params[\"env\"],observation_keys=(\"glyphs\",\"glyphs_crop\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "            actions=actions) #create the environment\n",
        "\n",
        "\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    # Create dqn agent\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    agent = DQNAgent(observation_space = env.observation_space,\n",
        "                    action_space = env.action_space,\n",
        "                    replay_buffer = replay_buffer,\n",
        "                    use_double_dqn= hyper_params[\"use-double-dqn\"],\n",
        "                    lr = hyper_params['learning-rate'],\n",
        "                    batch_size = hyper_params['batch-size'],\n",
        "                    gamma = hyper_params['discount-factor'] )\n",
        "\n",
        "    # Traine the agent\n",
        "    TrainDQN(\n",
        "      env,\n",
        "      agent,\n",
        "      max_timesteps=hyper_params['num-steps'],\n",
        "      hyper_params=hyper_params,\n",
        "      replay_buffer=replay_buffer\n",
        "    )"
      ],
      "metadata": {
        "id": "q-BwIiYsgiKA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the agent for several runs on this environment\n",
        "num_runs=5\n",
        "for i in range(num_runs):\n",
        "  print(\"run: {}\".format(i))\n",
        "  RunDQN1()"
      ],
      "metadata": {
        "id": "epVaVn-mg-yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-Task 2: MiniHack-MazeWalk-9x9-v0"
      ],
      "metadata": {
        "id": "AcqeipRah_bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RunDQN2():\n",
        "    hyper_params = {\n",
        "    \"seed\": 42,  # which seed to use\n",
        "    \"env\": \"MiniHack-MazeWalk-9x9-v0\",  # name of the game\n",
        "    \"replay-buffer-size\": int(5e4),  # replay buffer size\n",
        "    \"learning-rate\": 1e-3,  # learning rate for Adam optimizer\n",
        "    \"discount-factor\": 0.99,  # discount factor\n",
        "    \"num-steps\": 500000,  # total number of steps to run the environment for\n",
        "    \"batch-size\": 64,  # number of transitions to optimize at the same time\n",
        "    \"learning-starts\": 5000,  # number of steps before learning starts\n",
        "    \"learning-freq\": 2,  # number of iterations between every optimization step\n",
        "    \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "    \"target-update-freq\": 100,  # number of iterations between every target network update\n",
        "    \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "    \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "    \"eps-fraction\": 0.3,  # fraction of num-steps\n",
        "    \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    actions = tuple(nethack.CompassDirection) #actions restricted to sufficient ones needed for this environment\n",
        "\n",
        "    env = gym.make(hyper_params[\"env\"],observation_keys=(\"glyphs\",\"glyphs_crop\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "            actions=actions) #create the environment\n",
        "\n",
        "\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    #Create dqn agent\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    agent = DQNAgent(observation_space = env.observation_space,\n",
        "                    action_space = env.action_space,\n",
        "                    replay_buffer = replay_buffer,\n",
        "                    use_double_dqn= hyper_params[\"use-double-dqn\"],\n",
        "                    lr = hyper_params['learning-rate'],\n",
        "                    batch_size = hyper_params['batch-size'],\n",
        "                    gamma = hyper_params['discount-factor'] )\n",
        "\n",
        "    # Train agent\n",
        "    TrainDQN(\n",
        "      env,\n",
        "      agent,\n",
        "      max_timesteps=hyper_params['num-steps'],\n",
        "      hyper_params=hyper_params,\n",
        "      replay_buffer=replay_buffer\n",
        "    )"
      ],
      "metadata": {
        "id": "BpnllSc0iGSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the agent for several runs on this environment\n",
        "num_runs=5\n",
        "for i in range(num_runs):\n",
        "  print(\"run: {}\".format(i))\n",
        "  RunDQN2()"
      ],
      "metadata": {
        "id": "R7CRRjnBiRxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-Task 3: MiniHack-LockedDoor-v0"
      ],
      "metadata": {
        "id": "Mx-9NOdGiY3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RunDQN3():\n",
        "    hyper_params = {\n",
        "    \"seed\": 42,  # which seed to use\n",
        "    \"env\": \"MiniHack-LockedDoor-v0\",  # name of the game\n",
        "    \"replay-buffer-size\": int(5e4),  # replay buffer size\n",
        "    \"learning-rate\": 1e-3,  # learning rate for Adam optimizer\n",
        "    \"discount-factor\": 0.99,  # discount factor\n",
        "    \"num-steps\": 500000,  # total number of steps to run the environment for\n",
        "    \"batch-size\": 64,  # number of transitions to optimize at the same time\n",
        "    \"learning-starts\": 5000,  # number of steps before learning starts\n",
        "    \"learning-freq\": 2,  # number of iterations between every optimization step\n",
        "    \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "    \"target-update-freq\": 100,  # number of iterations between every target network update\n",
        "    \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "    \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "    \"eps-fraction\": 0.3,  # fraction of num-steps\n",
        "    \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    actions = tuple(nethack.CompassDirection) #actions restricted to sufficient ones needed for this environment\n",
        "    actions+=(nethack.Command.KICK,)\n",
        "\n",
        "    env = gym.make(hyper_params[\"env\"],observation_keys=(\"glyphs\",\"glyphs_crop\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "            actions=actions) #create the environment\n",
        "\n",
        "\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    #Create dqn agent\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    agent = DQNAgent(observation_space = env.observation_space,\n",
        "                    action_space = env.action_space,\n",
        "                    replay_buffer = replay_buffer,\n",
        "                    use_double_dqn= hyper_params[\"use-double-dqn\"],\n",
        "                    lr = hyper_params['learning-rate'],\n",
        "                    batch_size = hyper_params['batch-size'],\n",
        "                    gamma = hyper_params['discount-factor'] )\n",
        "\n",
        "    # Train agent\n",
        "    TrainDQN(\n",
        "      env,\n",
        "      agent,\n",
        "      max_timesteps=hyper_params['num-steps'],\n",
        "      hyper_params=hyper_params,\n",
        "      replay_buffer=replay_buffer\n",
        "    )"
      ],
      "metadata": {
        "id": "MLzTt_5OieRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the agent for several runs on this environment\n",
        "num_runs=5\n",
        "for i in range(num_runs):\n",
        "  print(\"run: {}\".format(i))\n",
        "  RunDQN3()"
      ],
      "metadata": {
        "id": "E2Rqnj0wir5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Task: MiniHack-Quest-Hard-v0"
      ],
      "metadata": {
        "id": "ghfmolZDjayt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RunDQN4():\n",
        "    hyper_params = {\n",
        "    \"seed\": 42,  # which seed to use\n",
        "    \"env\": \"MiniHack-Quest-Hard-v0\",  # name of the game\n",
        "    \"replay-buffer-size\": int(5e4),  # replay buffer size\n",
        "    \"learning-rate\": 1e-3,  # learning rate for Adam optimizer\n",
        "    \"discount-factor\": 0.99,  # discount factor\n",
        "    \"num-steps\": 500000,  # total number of steps to run the environment for\n",
        "    \"batch-size\": 64,  # number of transitions to optimize at the same time\n",
        "    \"learning-starts\": 5000,  # number of steps before learning starts\n",
        "    \"learning-freq\": 2,  # number of iterations between every optimization step\n",
        "    \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "    \"target-update-freq\": 100,  # number of iterations between every target network update\n",
        "    \"eps-start\": 1.0,  # e-greedy start threshold\n",
        "    \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "    \"eps-fraction\": 0.3,  # fraction of num-steps\n",
        "    \"print-freq\": 10,\n",
        "    }\n",
        "\n",
        "    np.random.seed(hyper_params[\"seed\"])\n",
        "    random.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    actions = tuple(nethack.CompassDirection) #actions restricted to sufficient ones needed for this environment\n",
        "    actions=actions+(\n",
        "        nethack.Command.PICKUP,\n",
        "        nethack.Command.APPLY,\n",
        "        nethack.Command.FIRE,\n",
        "        nethack.Command.RUSH,\n",
        "        nethack.Command.ZAP,\n",
        "        nethack.Command.PUTON,\n",
        "        nethack.Command.READ,\n",
        "        nethack.Command.WEAR,\n",
        "        nethack.Command.QUAFF,\n",
        "        nethack.Command.PRAY,\n",
        "        nethack.Command.KICK,\n",
        "        )\n",
        "\n",
        "    env = gym.make(hyper_params[\"env\"],observation_keys=(\"glyphs\",\"glyphs_crop\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "            actions=actions) #create the environment\n",
        "\n",
        "\n",
        "    env.seed(hyper_params[\"seed\"])\n",
        "\n",
        "    replay_buffer = ReplayBuffer(hyper_params[\"replay-buffer-size\"])\n",
        "\n",
        "    #Create dqn agent\n",
        "\n",
        "    print(device)\n",
        "\n",
        "    agent = DQNAgent(observation_space = env.observation_space,\n",
        "                    action_space = env.action_space,\n",
        "                    replay_buffer = replay_buffer,\n",
        "                    use_double_dqn= hyper_params[\"use-double-dqn\"],\n",
        "                    lr = hyper_params['learning-rate'],\n",
        "                    batch_size = hyper_params['batch-size'],\n",
        "                    gamma = hyper_params['discount-factor'] )\n",
        "\n",
        "    # Train agent\n",
        "    TrainDQN(\n",
        "      env,\n",
        "      agent,\n",
        "      max_timesteps=hyper_params['num-steps'],\n",
        "      hyper_params=hyper_params,\n",
        "      replay_buffer=replay_buffer\n",
        "    )"
      ],
      "metadata": {
        "id": "QakULtqjjZ5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the agent for several runs on this environment\n",
        "num_runs=5\n",
        "for i in range(num_runs):\n",
        "  print(\"run: {}\".format(i))\n",
        "  RunDQN4()"
      ],
      "metadata": {
        "id": "xfQkVNgtj0dx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}