{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKdXs1tze3sU"
      },
      "outputs": [],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y build-essential autoconf libtool pkg-config python3-dev \\\n",
        "    python3-pip python3-numpy git flex bison libbz2-dev\n",
        "\n",
        "!wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | sudo apt-key add -\n",
        "!sudo apt-add-repository 'deb https://apt.kitware.com/ubuntu/ bionic main'\n",
        "!sudo apt-get update && apt-get --allow-unauthenticated install -y \\\n",
        "    cmake \\\n",
        "    kitware-archive-keyring\n",
        "\n",
        "!sudo rm $(which cmake)\n",
        "!$(which cmake) --version\n",
        "\n",
        "!pip3 install -Uv nle\n",
        "!apt-get install sox ffmpeg libcairo2 libcairo2-dev\n",
        "!pip install manimlib pygame opencv-python minihack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iPtEtHoHEcR"
      },
      "outputs": [],
      "source": [
        "# https://github.com/BY571/SAC_discrete/blob/main/train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPGDPTL85dbQ"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spLWye2_FP0b"
      },
      "source": [
        "Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7XduPH6FGl8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import minihack\n",
        "\n",
        "\n",
        "def hidden_init(layer):\n",
        "    fan_in = layer.weight.data.size()[0]\n",
        "    lim = 1. / np.sqrt(fan_in)\n",
        "    return (-lim, lim)\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hyperparams):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            fc1_units (int): Number of nodes in first hidden layer\n",
        "            fc2_units (int): Number of nodes in second hidden layer\n",
        "        \"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        # Initialize fully connected layers for glyph output after convolutional and pooling layers\n",
        "        self.hidden_size = hyperparams[\"hidden_size\"]\n",
        "        self.fc1 = Linear(state_size, out_features=self.hidden_size)\n",
        "        self.fc2 = Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = Linear(in_features=self.hidden_size, out_features=action_size)\n",
        "        # To calculate the probability of taking each action in the given state\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, state):\n",
        "       # Transform the glyph and state arrays into tensors\n",
        "\n",
        "        if (type(state) is dict): # if state is just a dict with 2 keys (glyphs, message)\n",
        "          message_t  = torch.from_numpy(state[\"message\"]).float().to(device)\n",
        "          glyphs_t  = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "\n",
        "          glyphs_t = glyphs_t.reshape((1,1659))\n",
        "\n",
        "\n",
        "        else: # if state is a batch - here, state is an array of dicts - if you call agent.learn()\n",
        "            glyphs_t  = torch.from_numpy(np.array([ s[\"glyphs\"] for s in state])).float().to(device)\n",
        "            message_t  = torch.from_numpy(np.array([ s[\"message\"] for s in state])).float().to(device)\n",
        "\n",
        "            glyphs_t = torch.squeeze(glyphs_t, 1) # remove all dimensions with 1\n",
        "            message_t = torch.squeeze(message_t, 1) # remove all dimensions with 1\n",
        "\n",
        "            # print(glyphs_t.shape)\n",
        "            glyphs_t = glyphs_t.reshape((256,1659))\n",
        "\n",
        "        # Combine glyphs output from convolution and fully connected layers\n",
        "        # with message output from fully connected layer\n",
        "        # Cat and Concat are used for different versions of PyTorch\n",
        "\n",
        "        try:\n",
        "            combined = torch.cat((glyphs_t,message_t),1)\n",
        "        except:\n",
        "            combined = torch.concat([glyphs_t,message_t],1)\n",
        "\n",
        "        # Pass glyphs and messaged combination through a fully connected layer\n",
        "\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        action_probs = self.softmax(self.fc3(x))\n",
        "        return action_probs\n",
        "\n",
        "    def evaluate(self, state, epsilon=1e-6):\n",
        "        action_probs = self.forward(state)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample()\n",
        "        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n",
        "        z = action_probs == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probs + z)\n",
        "        return action.detach().cpu(), action_probs, log_action_probabilities\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        returns the action based on a squashed gaussian policy. That means the samples are obtained according to:\n",
        "        a(s,e)= tanh(mu(s)+sigma(s)+e)\n",
        "        \"\"\"\n",
        "        action_probs = self.forward(state)\n",
        "\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample().to(device)\n",
        "        # Have to deal with situation of 0.0 probabilities because we can't do log 0\n",
        "        z = action_probs == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probs + z)\n",
        "        return action.detach().cpu(), action_probs, log_action_probabilities\n",
        "\n",
        "    def get_det_action(self, state):\n",
        "        action_probs = self.forward(state)\n",
        "        dist = Categorical(action_probs)\n",
        "        action = dist.sample().to(device)\n",
        "        return action.detach().cpu()\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"Critic (Value) Model.\"\"\"\n",
        "\n",
        "    def __init__(self, state_size, action_size, hyperparams):\n",
        "        \"\"\"Initialize parameters and build model.\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): Dimension of each state\n",
        "            action_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "            hidden_size (int): Number of nodes in the network layers\n",
        "        \"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        self.hidden_size = hyperparams[\"hidden_size\"]\n",
        "        # Initialize fully connected layers for glyph output after convolutional and pooling layers\n",
        "        self.fc1 = Linear(state_size, out_features=self.hidden_size)\n",
        "        self.fc2 = Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
        "        self.fc3 = Linear(in_features=self.hidden_size, out_features=action_size)\n",
        "        # To calculate the probability of taking each action in the given state\n",
        "        # self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
        "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
        "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
        "\n",
        "    def forward(self, state):\n",
        "       # Transform the glyph and state arrays into tensors\n",
        "\n",
        "        if (type(state) is dict): # if state is just a dict with 2 keys (glyphs, message)\n",
        "          message_t  = torch.from_numpy(state[\"message\"]).float().to(device)\n",
        "          glyphs_t  = torch.from_numpy(state[\"glyphs\"]).float().to(device)\n",
        "\n",
        "          glyphs_t = glyphs_t.reshape((1,1659))\n",
        "\n",
        "        else: # if state is a batch - here, state is an array of dicts - if you call agent.learn()\n",
        "            glyphs_t  = torch.from_numpy(np.array([ s[\"glyphs\"] for s in state])).float().to(device)\n",
        "            message_t  = torch.from_numpy(np.array([ s[\"message\"] for s in state])).float().to(device)\n",
        "\n",
        "            glyphs_t = torch.squeeze(glyphs_t, 1) # remove all dimensions with 1\n",
        "            message_t = torch.squeeze(message_t, 1) # remove all dimensions with 1\n",
        "\n",
        "            glyphs_t = glyphs_t.reshape((256,1659))\n",
        "\n",
        "        # Combine glyphs output from convolution and fully connected layers\n",
        "        # with message output from fully connected layer\n",
        "        # Cat and Concat are used for different versions of PyTorch\n",
        "        try:\n",
        "            combined = torch.cat((glyphs_t,message_t),1)\n",
        "        except:\n",
        "            combined = torch.concat([glyphs_t,message_t],1)\n",
        "\n",
        "        # Pass glyphs and messaged combination through a fully connected layer\n",
        "\n",
        "        x = F.relu(self.fc1(combined))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rgIgZ6XFUTu"
      },
      "source": [
        "Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qI_KDyJpFWQp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import copy\n",
        "\n",
        "\n",
        "class SAC(nn.Module):\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                        state_size,\n",
        "                        action_size,\n",
        "                        hyperparams,\n",
        "                        device\n",
        "                ):\n",
        "        \"\"\"Initialize an Agent object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            state_size (int): dimension of each state\n",
        "            action_size (int): dimension of each action\n",
        "            random_seed (int): random seed\n",
        "        \"\"\"\n",
        "        super(SAC, self).__init__()\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.gamma = hyperparams[\"discount\"]\n",
        "        self.tau = hyperparams[\"interpolation_factor\"]\n",
        "        hidden_size = hyperparams[\"hidden_size\"]\n",
        "        learning_rate = hyperparams[\"lr\"]\n",
        "        self.clip_grad_param = hyperparams[\"clip_grad_param\"]\n",
        "\n",
        "        self.target_entropy = -action_size  # -dim(A)\n",
        "\n",
        "        self.log_alpha = torch.tensor([0.0], requires_grad=True)\n",
        "        self.alpha = self.log_alpha.exp().detach()\n",
        "        self.alpha_optimizer = optim.Adam(params=[self.log_alpha], lr=learning_rate)\n",
        "\n",
        "        # Actor Network\n",
        "\n",
        "        self.actor_local = Actor(state_size, action_size, hyperparams).to(device)\n",
        "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=learning_rate)\n",
        "\n",
        "        # Critic Network (w/ Target Network)\n",
        "\n",
        "        self.critic1 = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic2 = Critic(state_size, action_size, hyperparams).to(device)\n",
        "\n",
        "        assert self.critic1.parameters() != self.critic2.parameters()\n",
        "\n",
        "        self.critic1_target = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
        "\n",
        "        self.critic2_target = Critic(state_size, action_size, hyperparams).to(device)\n",
        "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
        "        # state = torch.from_numpy(state).float().to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action = self.actor_local.get_det_action(state)\n",
        "        return action.numpy()\n",
        "\n",
        "    def calc_policy_loss(self, states, alpha):\n",
        "        _, action_probs, log_pis = self.actor_local.evaluate(states)\n",
        "\n",
        "        q1 = self.critic1(states)\n",
        "        q2 = self.critic2(states)\n",
        "        min_Q = torch.min(q1,q2)\n",
        "        actor_loss = (action_probs * (alpha * log_pis - min_Q )).sum(1).mean()\n",
        "        log_action_pi = torch.sum(log_pis * action_probs, dim=1)\n",
        "        return actor_loss, log_action_pi\n",
        "\n",
        "    def learn(self, step, experiences, gamma, d=1):\n",
        "        \"\"\"Updates actor, critics and entropy_alpha parameters using given batch of experience tuples.\n",
        "        Q_targets = r + γ * (min_critic_target(next_state, actor_target(next_state)) - α *log_pi(next_action|next_state))\n",
        "        Critic_loss = MSE(Q, Q_target)\n",
        "        Actor_loss = α * log_pi(a|s) - Q(s,a)\n",
        "        where:\n",
        "            actor_target(state) -> action\n",
        "            critic_target(state, action) -> Q-value\n",
        "        Params\n",
        "        ======\n",
        "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "\n",
        "        # ---------------------------- update actor ---------------------------- #\n",
        "        current_alpha = copy.deepcopy(self.alpha)\n",
        "        actor_loss, log_pis = self.calc_policy_loss(states, current_alpha.to(self.device))\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Compute alpha loss # α > 0 is called the temperature and determines the trade-off between received rewards and randomness of the policy\n",
        "        alpha_loss = - (self.log_alpha.exp() * (log_pis.cpu() + self.target_entropy).detach().cpu()).mean() # temperature cost function\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "        self.alpha = self.log_alpha.exp().detach()\n",
        "\n",
        "        # ---------------------------- update critic ---------------------------- #\n",
        "        # Get predicted next-state actions and Q values from target models\n",
        "        with torch.no_grad():\n",
        "            _, action_probs, log_pis = self.actor_local.evaluate(next_states)\n",
        "            Q_target1_next = self.critic1_target(next_states)\n",
        "            Q_target2_next = self.critic2_target(next_states)\n",
        "            Q_target_next = action_probs * (torch.min(Q_target1_next, Q_target2_next) - self.alpha.to(self.device) * log_pis)\n",
        "\n",
        "            # Compute Q targets for current states (y_i)\n",
        "            Q_targets = rewards + (gamma * (1 - dones) * Q_target_next.sum(dim=1).unsqueeze(-1))\n",
        "\n",
        "        # Compute critic loss\n",
        "        q1 = self.critic1(states).gather(1, actions.long()) # Gathers values along an axis specified by dim. input_tensor.gather(dim, index) . index is long tensor.\n",
        "        q2 = self.critic2(states).gather(1, actions.long()) # input and index must be same dimension\n",
        "\n",
        "        critic1_loss = 0.5 * F.mse_loss(q1, Q_targets)\n",
        "        critic2_loss = 0.5 * F.mse_loss(q2, Q_targets)\n",
        "\n",
        "        # Update critics\n",
        "        # critic 1\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        critic1_loss.backward(retain_graph=True)\n",
        "        clip_grad_norm_(self.critic1.parameters(), self.clip_grad_param)\n",
        "        self.critic1_optimizer.step()\n",
        "        # critic 2\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic2_loss.backward()\n",
        "        clip_grad_norm_(self.critic2.parameters(), self.clip_grad_param)\n",
        "        self.critic2_optimizer.step()\n",
        "\n",
        "        # ----------------------- update target networks ----------------------- #\n",
        "        self.soft_update(self.critic1, self.critic1_target)\n",
        "        self.soft_update(self.critic2, self.critic2_target)\n",
        "\n",
        "        return actor_loss.item(), alpha_loss.item(), critic1_loss.item(), critic2_loss.item(), current_alpha\n",
        "\n",
        "    def soft_update(self, local_model , target_model):\n",
        "        \"\"\"Soft update model parameters.\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
        "        Params\n",
        "        ======\n",
        "            local_model: PyTorch model (weights will be copied from)\n",
        "            target_model: PyTorch model (weights will be copied to)\n",
        "            tau (float): interpolation parameter\n",
        "        \"\"\"\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format state"
      ],
      "metadata": {
        "id": "SCip6rKVasiO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9G10Jrumv6r"
      },
      "outputs": [],
      "source": [
        "def format_state(state):\n",
        "    \"\"\"Formats the state according to the input requirements of the Actor Critic Neural Network\"\"\"\n",
        "\n",
        "    # Normalize and reshape for convolutional layer input\n",
        "    glyphs = state[\"glyphs\"]\n",
        "    glyphs = glyphs/glyphs.max()\n",
        "    glyphs = glyphs.reshape((1,1,21,79))\n",
        "\n",
        "    # Normalize the message and reshape for the fully connected layer input\n",
        "    message = state[\"message\"]\n",
        "    if state[\"message\"].max()>0:\n",
        "        # Occassionally the message is empty which will cause a Zero Division error\n",
        "        message = message/message.max()\n",
        "    message = message.reshape((1,len(message)))\n",
        "\n",
        "    state = {\"glyphs\":glyphs,\"message\":message}\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxUOvK8ZINgB"
      },
      "source": [
        "Collecting samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOqfhwG6IOto"
      },
      "outputs": [],
      "source": [
        "def collect_random(env, dataset, num_samples):\n",
        "    state = format_state(env.reset())\n",
        "    for _ in range(num_samples):\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = format_state(next_state)\n",
        "        dataset.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        if done:\n",
        "            state = format_state(env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUHVE9whmsss"
      },
      "source": [
        "Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk4bGu_nijPn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size, device):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Params\n",
        "        ======\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = np.stack([e.state for e in experiences if e is not None])\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = np.stack([e.next_state for e in experiences if e is not None])\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uvl1JS2RFjRu"
      },
      "source": [
        "Training Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmI09_BlFiou"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch\n",
        "import argparse\n",
        "import glob\n",
        "import random\n",
        "from nle import nethack\n",
        "\n",
        "\n",
        "hyperparams = {\n",
        "      \"run_name\": \"SAC Discrete\",\n",
        "      # \"env_name\": \"MiniHack-MazeWalk-9x9-v0\",\n",
        "      \"env_name\": \"MiniHack-Room-5x5-v0\",\n",
        "      \"episodes\": 500,\n",
        "      \"buffer_size\": int(1e6),\n",
        "      \"seed\": 42,\n",
        "      \"log_video\": 0,\n",
        "      \"save_every\": 100,\n",
        "      \"batch_size\": 256,\n",
        "      \"discount\":0.99, # discount factor gamma\n",
        "      \"lr\": 2e-4, # learning rate alpha\n",
        "      # \"lr\": 0.02, # learning rate alpha\n",
        "      \"hidden_size\": 256, # hidden layer size\n",
        "      \"interpolation_factor\": 0.005,   #tau - for soft update\n",
        "      \"clip_grad_param\":1, # gradient clipping\n",
        "      \"max_episode_steps\":1000\n",
        "\n",
        "  }\n",
        "\n",
        "\n",
        "np.random.seed(hyperparams[\"seed\"])\n",
        "random.seed(hyperparams[\"seed\"])\n",
        "torch.manual_seed(hyperparams[\"seed\"])\n",
        "\n",
        "MOVE_ACTIONS = tuple(nethack.CompassDirection) + (\n",
        "                nethack.Command.OPEN,\n",
        "                nethack.Command.KICK\n",
        "            )\n",
        "env = gym.make(hyperparams[\"env_name\"],observation_keys=(\"glyphs\", \"chars\", \"colors\", \"pixel\", \"message\", \"blstats\", \"pixel_crop\"),\n",
        "        actions=MOVE_ACTIONS,max_episode_steps=hyperparams[\"max_episode_steps\"])\n",
        "\n",
        "env.seed(hyperparams[\"seed\"])\n",
        "env.action_space.seed(hyperparams[\"seed\"])\n",
        "torch.cuda.manual_seed_all(hyperparams[\"seed\"])\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "steps = 0\n",
        "average10 = deque(maxlen=10)\n",
        "total_steps = 0\n",
        "\n",
        "agent = SAC(state_size=1915,\n",
        "                  action_size=len(MOVE_ACTIONS),\n",
        "                  hyperparams=hyperparams,\n",
        "                  device=device)\n",
        "\n",
        "\n",
        "buffer = ReplayBuffer(buffer_size=hyperparams[\"buffer_size\"], batch_size=hyperparams[\"batch_size\"], device=device )\n",
        "\n",
        "collect_random(env=env, dataset=buffer, num_samples=10000) # generate random samples\n",
        "\n",
        "rewards_arr = []\n",
        "policy_loss_arr = []\n",
        "critic1_loss_arr = []\n",
        "\n",
        "if hyperparams[\"log_video\"]:\n",
        "    env = gym.wrappers.Monitor(env, './video', video_callable=lambda x: x%10==0, force=True)\n",
        "\n",
        "for i in range(1, hyperparams[\"episodes\"]+1):\n",
        "    state = format_state(env.reset())\n",
        "\n",
        "    episode_steps = 0\n",
        "    rewards = 0\n",
        "    print(i)\n",
        "    while True:\n",
        "        action = agent.get_action(state)\n",
        "        steps += 1\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "        next_state = format_state(next_state)\n",
        "\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "\n",
        "        policy_loss, alpha_loss, bellmann_error1, bellmann_error2, current_alpha = agent.learn(steps, buffer.sample(), hyperparams[\"discount\"])\n",
        "        state = next_state\n",
        "        rewards += reward\n",
        "        episode_steps += 1\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    average10.append(rewards)\n",
        "    total_steps += episode_steps\n",
        "    print(\"Episode: {} | Reward: {} | Policy Loss: {} | Steps: {}\".format(i, rewards, policy_loss, steps,))\n",
        "    rewards_arr.append(rewards)\n",
        "    policy_loss_arr.append(policy_loss)\n",
        "    critic1_loss_arr.append(bellmann_error1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/rewards.txt', rewards_arr)\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/policy_loss.txt', policy_loss_arr)\n",
        "np.savetxt('/content/drive/MyDrive/reinforcement_learning/maze_sac_minihack_nn/critic1_loss.txt', critic1_loss_arr)\n",
        "print(\"total number of steps: \", steps)"
      ],
      "metadata": {
        "id": "_CSx8UPX9ExG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U71v5IeQU-na"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rewards_arr)\n",
        "plt.title('Average Reward on Room-5x5-v0')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkVxGGlJVCGi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(policy_loss_arr)\n",
        "plt.title('Policy Loss on Room-5x5-v0')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(critic1_loss_arr)\n",
        "plt.title('Critic 1 Loss on Room=5x5-v0')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Episode')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ny7Wk56hVoZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}